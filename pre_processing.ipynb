{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = [\"@handle\", \"RT\", \"http\"]\n",
    "\n",
    "def pre_process(sentence, max_length):\n",
    "    sentence = sentence.split()\n",
    "    target_remove = set()\n",
    "    for token in sentence:\n",
    "        for target in remove_words:\n",
    "            if (target == \"http\") and (target in token.lower()):\n",
    "                target_remove.add(token)\n",
    "                break\n",
    "            if target in token:\n",
    "                target_remove.add(token)\n",
    "                break  \n",
    "    for target in target_remove:\n",
    "        sentence.remove(target)\n",
    "    max_length = max(max_length, len(sentence))\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence, max_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Train data to train_dict. \n",
    "train_dict[id] = [[train_instace1], [train_instance2] ....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 328931\n",
      "Total ids: 9296\n",
      "Longest Sentence: 37\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "train_file_path = \"data/train_tweets.txt\"\n",
    "train_dict = collections.defaultdict(list)\n",
    "max_length = 0\n",
    "\n",
    "length_array = []\n",
    "with open(train_file_path, encoding='utf-8') as tsvfile:\n",
    "    reader = tsvfile.readlines()\n",
    "    for i, row in enumerate(reader):\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        id = int(row[0])\n",
    "        instance, max_length = pre_process(row[1], max_length)\n",
    "        if not instance == \"\":\n",
    "            train_dict[id].append(instance)\n",
    "            length_array.append(len(instance.split()))\n",
    "    print(\"Total rows: %d\" % i)\n",
    "    \n",
    "print(\"Total ids: %d\" % len(train_dict))\n",
    "print(\"Longest Sentence: %d\" % (max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: [ 8 11  6 ... 12 10 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    327981.000000\n",
       "mean         12.851967\n",
       "std           6.644516\n",
       "min           1.000000\n",
       "25%           8.000000\n",
       "50%          12.000000\n",
       "75%          18.000000\n",
       "max          37.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "length_array = np.array(length_array)\n",
    "sorted_len_array = sorted(length_array)\n",
    "print(\"median: {0}\".format(length_array))\n",
    "s = pd.Series(length_array)\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tweetsPerPersonMap = {}\n",
    "for key in train_dict.keys():\n",
    "    tweetsLen = len(train_dict[key])\n",
    "    if tweetsLen not in tweetsPerPersonMap.keys():\n",
    "        tweetsPerPersonMap[tweetsLen] = [key]\n",
    "    else:\n",
    "        tweetsPerPersonMap[tweetsLen].append(key)\n",
    "\n",
    "sortedTweetsNumList = sorted(tweetsPerPersonMap.keys())\n",
    "sortedTweetsSumList = []\n",
    "userSumList = []\n",
    "for tweetsNum in sortedTweetsNumList:\n",
    "    userNum = len(tweetsPerPersonMap[tweetsNum])\n",
    "    sortedTweetsSumList.append(tweetsNum*userNum)\n",
    "    userSumList.append(userNum)\n",
    "\n",
    "sortedTweetsSumList = np.cumsum(sortedTweetsSumList)\n",
    "userSumList = np.cumsum(userSumList)\n",
    "userSumPropotionList = userSumList/userSumList[-1]\n",
    "sortedTweetsProportionList = sortedTweetsSumList/sortedTweetsSumList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tweets num per person\n",
    "tweets_num_path = 'data/v1_1/tweets_num_per_person.txt'\n",
    "with open(tweets_num_path, 'w') as txtFile:\n",
    "    for index, tweetsNum in enumerate(sortedTweetsNumList):\n",
    "        userList = tweetsPerPersonMap[tweetsNum]\n",
    "        proportion = sortedTweetsProportionList[index]\n",
    "        userProportion = userSumPropotionList[index]\n",
    "        #txtFile.write(\"{0:.2f},{1:.2f},{2},{3}\\n\".format(proportion, userProportion, tweetsNum, userList))\n",
    "        txtFile.write(\"{0:.2f},{1:.2f},{2}\\n\".format(proportion, userProportion, tweetsNum, userList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "# store new dict\n",
    "TWEETS_NUM_THRESHOLD= 55\n",
    "\n",
    "sortedTweetsNumList = np.array(sortedTweetsNumList)\n",
    "newTweetsNumList = sortedTweetsNumList[sortedTweetsNumList>TWEETS_NUM_THRESHOLD]\n",
    "\n",
    "print(len(newTweetsNumList))\n",
    "\n",
    "newTrainDict = {}\n",
    "for index, tweetsNum in enumerate(newTweetsNumList):\n",
    "    for user in tweetsPerPersonMap[tweetsNum]:\n",
    "        newTrainDict[user] = train_dict[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132 1132 1132\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "dev_split = 0.1\n",
    "train_split = 1 - dev_split\n",
    "\n",
    "dev_set_dict = {}\n",
    "train_set_dict = {}\n",
    "\n",
    "for id in newTrainDict:\n",
    "    target_list = newTrainDict[id]\n",
    "    length = len(target_list)\n",
    "    random.shuffle(target_list)\n",
    "    split = int(np.ceil(length*dev_split))\n",
    "    dev_set_dict[id] = target_list[:split]\n",
    "    train_set_dict[id] = target_list[split:length]\n",
    "#     print(len(dev_set_dict[id]), len(train_set_dict[id]), length)\n",
    "\n",
    "print(len(dev_set_dict), len(train_set_dict), len(newTrainDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11135\n",
      "95451\n",
      "1132\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_file(target_dict, file_path):\n",
    "    id_list = []\n",
    "    sentence_list = []\n",
    "    for id in target_dict:\n",
    "        for sentence in target_dict[id]:\n",
    "            id_list.append(id)\n",
    "            sentence_list.append(sentence)\n",
    "      \n",
    "    id_list = np.array(id_list)\n",
    "    sentence_list = np.array(sentence_list)\n",
    "    random_index = np.array(range(len(sentence_list)))\n",
    "    random.shuffle(random_index)\n",
    "    id_list = id_list[random_index]\n",
    "    sentence_list = sentence_list[random_index]\n",
    "    \n",
    "    with open(file_path,'w') as output_file:\n",
    "        for i in range(len(id_list)):\n",
    "            result = \"{0}\\t{1}\\n\".format(id_list[i], sentence_list[i])\n",
    "            output_file.write(result)\n",
    "    #dataframe = pd.DataFrame({'id':id_list,'sentence':sentence_list})\n",
    "    #dataframe.to_csv(file_path,index=False,sep='\\t',header=None)\n",
    "    print(len(id_list))\n",
    "    return\n",
    "\n",
    "dev_set_path = 'data/v1_1/dev_set_v1_1_more_than_{0}.txt'.format(TWEETS_NUM_THRESHOLD)\n",
    "train_set_path = 'data/v1_1/train_set_v1_1_more_than_{0}.txt'.format(TWEETS_NUM_THRESHOLD)\n",
    "idx_file_path = 'data/v1_1/v1_idx_more_than_{0}.pickle'.format(TWEETS_NUM_THRESHOLD)\n",
    "\n",
    "save_to_file(dev_set_dict, dev_set_path)\n",
    "save_to_file(train_set_dict, train_set_path)\n",
    "\n",
    "\n",
    "# Build IDX and Save\n",
    "idx = {}\n",
    "for i, id in enumerate(newTrainDict):\n",
    "    idx[id] = i\n",
    "print(len(idx))\n",
    "\n",
    "with open(idx_file_path, 'wb') as handle:\n",
    "    pickle.dump(idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@handle Hahaha!', 'You look like Dave Vanian now? =D', \"Oh we renamed Zero, Seamus O'Malley - he looks like he should have a newsboy cap on & a pint of Guinness by his side...\", 'Dang, I keep missing all of the #bpal contests - I need a pick me up - this week already = fail.', \"I'm hungry! Just sayin'!\", 'My cousin said, \"Sea Mus\" - I know that\\'s not unusual to think that but it made me chuckle...', 'And Elena is still in the hospital too, obviously not as bad off as my dad, but my mom told me she & dad passed each other getting CT scans.', 'Most #atheists know the truth about Mother Teresa anyway...', '& esp. if they live in poverty, fuck the parents & child b/c the child ONLY mattered when it was a mass of cells in a womb. Wow - so smart.', 'Just saw it! Thank you, Tired & not in any mood to \"talk\" - at least I\\'m feeling more human today...*hugs*', 'BTW, J ?s his lil knitted monster, Toby, Sascha will bat it to the floor & J will go get it & put it back up on the clock.', \"I hope Squeegee & kitty (forgot its name!) are getting along! &lt;3 Sascha & mom's cat, Boots, hate each other; Glad we're dwnstrs.\", 'Btw: My brother, a \"proud\" meateater - ate tofurkey and was none the wiser - ha ha ha. I didn\\'t spill the beans \\'cause he\\'d be smug.', \"@handle I don't know because J is the house husband; I don't do a thing really! I'm so awful. I mean I do, but he does more!\", \"That NE album has one of my fave songs of theirs on it and My copy of Xymox's Headclouds was Fux0red. You're the best! <3 <3\", \"Dad update for anyone who cares: He's stable, death certificate was waiting for him in ER when ambulance arrived even though he was alive.\", '@handle Awww! Squeegee does great tricks! =D', \"@handle *SMOOCHES* to my best girls. ? 's you both so. Just saying.\", '- Elena is in the hospital too - 2 floors under my dad. Poor sweetie!', '- Pumpkin beer & my two faves!! Thank you Whole Foods in Chapel Hill!!!!', 'has discovered the wonderful world of #BPAL via Cthulu. Scientist & Athiest + BPAL = LOVE. &lt;3 Someone should send him a bottle!', '@handle J says, \"can I run through the middle of this convo nekkid?????\"', 'Pumpkin Cheesecake for the win!! Yum! =D', 'Seriously people YOUR = ownership, as in YOUR DOG. YOU\\'RE = YOU ARE, as in You\\'re going insane - get it, \"you are going insane.\" Gah!', 'We took a meal to my daddy in the nursing home - and he gobbled (ha ha) it down - even the tofurkey. J made everything & it was delicious!', 'Ok, done for the day with internet. Too tired to deal.', 'To those leaning right, did you ever read anything about Eugenics? How about Eugenics in the USA? It happened.', 'Dad had seizures start-ambulance took him to hospital-situation dire i am afraid.', \"J put up all of my mother's Xmas decorations - it looks like Santa exploded - I'll have to take a pic at some point tonight - no words.\", \"Haha, he'd love that... ;) Glad I don't have one! That keeps me from getting on Skype too - phew!\", 'Also, I has teh gas. I blame the stuffing. Was that TMI? OH WELLS!', 'In somewhat unrelated news, Happy-being-a-daddy-and-much-baby-love to & his wife. #babyfidalgo', 'Because it is! =D', \"Tofurkey - anyone who says it tastes bad apparently can't make a Turkey taste good or anything else for that matter!\", \"*nod* Peter is a mad genious; just sad he lost the patent and now everyone makes 'em; his purses were the best! :(\", \"So they're still not sure what's wrong with Elena - so tonight I'll be visiting both she & my father on diff. floors - I'm not there today.\", 'I will have a brand new baby girl in about 12 hours from now. | Congrats early Ben!!! <3', 'Not sure if he will survive the night...heartbroken. #h1n1', 'Nice surprise! Earlier today I found out Von Erickson is on Etsy! I LOVE my metal coffin purses! Old school goth for the win!', '#STUPAK = #STUPID. Please \"abort\" yourselves, STUPAK supporters. Kthnx. I\\'m tired of your religious stupidity. #atheism #hcr', 'Ni Ni Maya ? you *smooch*', \"It's to my shoulders but it's driving me batty b/c it's just boring!!!!!!!! :/\", '9-9-9: The Number of the Beatles -- | for @handle & @handle - a few days late but still!', 'Mooooooooooooooooooooo.', \"@handle Awwwww! I wrotes in the FaceBook that I was thankfuls for yous ladies on TDay. I am. You're both total tops! ?\", 'W E I R D! but not really. ;)', \"*nod* It's just the damn paypal & ebay fees eat me alive. I miscalculated S&H for one shipment & I had to bite it too...\", 'Amanda Palmer (of The Dresden Dolls) and Nervous Cabaret will play The ArtsCenter on Friday, November 20th', \"Happy Thanksgiving peeps! J is making all the food. Everything smells effing fabulous. I'm still in my PJs.\", 'Ok, I\\'m over the idiotic & stupid. Off to sell my clothing, or try, for funds because $$$ is everyone\\'s real, true \"God\".', \"Apparently dad is going to have an endoscopy - he's still spitting up blood & they're going to give him 2 pints of it today.\", 'Awwww! Hope Lo feels better fast! Candy canes, candy corn, cotton candy, & syrup! ?', \"Dear you totally did not have to do that but it is UBER appreciated - you've no idea. no idea. <3 <3 <3 <3 <3\", 'Father is doing bad. H1N1 is kicking his arse. Age 65 + spinal cord injury = worse problems. Meanwhile mom is bat shit insane.', \"My Democrat friends, please fill me in tomorrow on the goings on b/c I'm on my way to bed; over tonight & people in general Love ya'll tho.\", 'My mother told J he could paint him...HA HA HA - J said to me, we could make him Damien - paint a lil \"666\" in his hair & make his eyes red.', 'Church of Euthanasia could use a few good, cold bodies.', 'Not sure exactly what the test results will show. Not sure if he had a mild heart-attack or not even though they d (cont)', \"Mischka has an irritated rectum. J gave her applesauce mixed with Olive Leaf. Nature's cure we hope.\", 'I did too. Lots of onions, carrots, potatoes, sweet potatoes, fresh cranberry sauce with orange & apple, & tofurkey.', \"I would really love some Starbucks coffee (b/c it's the only decent coffee place in this one horse town) & I has no money = suck.NEED COFFEE\", \"I've photographed a gaggle of clothing & pulled out a metal coffin purse to sell along with a pair of Gripfast 30 eye (modified to 24) boots\", \"Thank you for trying but I am in no mood to talk on the phone - I'm exhausted physically & emotionally.\", 'I second that!', 'The pious princess masturbates & God watches.', \"Virginia & North Carolina's state governments allowed sterilization for ppl that were mainly black & poor & autistic AND White.\", 'I feel ick. I ate too many cookies & drank too much soy nog & rum.', '@handle You should totally video the cats doing tricks - so cute!', 'Well, at least my family has three atheists in it - I can somewhat get through the holidaze w/out drinking too much!', 'They are probably the ones who also consider them goths - I hate them so, hate hate hate.', 'and also win.', 'Dear Scientology loons & goons: Your TV commercials have become more annoying than the Latter Day Saints commercials. Thnx for that! Not.', \"I'm sorry that tweet was just wayyyyyyy too funny!\", 'People are indeed stupid.', 'Pro-Life\\'ers & ppl who call themselves \"Pro-Choice\" but don\\'t \"BELIEVE\" in abortion are so caught up in make-believe ppl. in the sky', \"@handle As long as it's close to feeding time, she does that & is just happy to have her bowl full until she's ready to eat!\", \"@handle Dogs are ridiculous. Sascha just meows at us until one of us gets up...usually J, I'm lazy. =D\", \"@handle That's funny b/c Mischka will sit & stamp her feet& *cry* until she gets food in her bowl, even when she's not hungry\", 'I meant to email you about that earlier! I swear my brain was in academic mode all day today! YES! THANK YOU SO MUCH!', 'About what? I\\'ve lost my temper today about the #abortion debate and health care \"reform\" - I\\'m sick of it all. :/', 'LOL!', \"Dear Rob Pattinson - I admit it, you're one effing hot guy. I watched Twilight again today. *swoon* Just saying!\", 'Gov. Elect Bob McDonnell \"Public option not good for Virginia\" - really?', 'And Paul doesn\\'t have to worry about my hunger b/c The Brides said it best: \"Baby Girls Are Much More Tender\".', \"Ppl. keep calling mother about my father & she puts on this fake cheery act; off phone she's a nervous wreck & i'm waiting for her to break.\", 'Some of my fave concerts from 1999/2000 were Apop concerts! =D And your blog has a great name! Yay for ADD! ;)', 'I keep reading that Eric Cantor is paying a lot of money for a speech coach. Who is that speech coach, Thirstin Howl III?', '- Bad band advertisement but saw it in Danville while in from Chapel Hill.', 'a;lfdkjal;sfjksafljk', 'Yikes! =D We had FIVE freaking dogs here on Turkey Day - gah! It was insanity - PLUS the two cats. Doggie politics...ugh.', \"I paid my bills & now I am teh brokes again. I'll be doing ebay again tomorrow. =D That's my job now apparently.\", 'Is someone walking Rep. Cao home tonight? I hope so. #protectcao #hcr || Had to RT that b4 bed - LOL. <3', \"I need my hair did. S'rsly. I don't know how much longer I can hold out!\", \"The best part? The nativity scene. There's a blonde haired, pale, blue-eyed baby Jesus. It's actually a NICE nativity set, not cheap.\", \"Last year Mary was dropped & broken & I cannot afford to replace her for mom - so one of the Magi is filling in. It's a gay nativity.\", 'I am full of sacrilege today.', \"Eek about the concussion! I'm glad you're ok-I had a friend's dog knock me in the head with his snout once & had that happen!\", 'Dear UNC if you could give me my refund - it would be \"totally awesome\" right now - kthnx.', '- New Message', 'I use logic, analysis, & reason - not emotion & anecdotal evidence. FA!']\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[4032])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dev Train\n",
    "!! There are Some ID have 0 train instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9296 9296 9296\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "dev_split = 0.1\n",
    "train_split = 1 - dev_split\n",
    "\n",
    "dev_set_dict = {}\n",
    "train_set_dict = {}\n",
    "\n",
    "for id in train_dict:\n",
    "    target_list = train_dict[id]\n",
    "    length = len(target_list)\n",
    "    random.shuffle(target_list)\n",
    "    split = int(np.ceil(length*dev_split))\n",
    "    dev_set_dict[id] = target_list[:split]\n",
    "    train_set_dict[id] = target_list[split:length]\n",
    "#     print(len(dev_set_dict[id]), len(train_set_dict[id]), length)\n",
    "\n",
    "print(len(dev_set_dict), len(train_set_dict), len(train_dict))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build ID idx and train/dev set save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36124\n",
      "291862\n",
      "9296\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_file(target_dict, file_path):\n",
    "    id_list = []\n",
    "    sentence_list = []\n",
    "    for id in target_dict:\n",
    "        for sentence in target_dict[id]:\n",
    "            id_list.append(id)\n",
    "            sentence_list.append(sentence)\n",
    "      \n",
    "    id_list = np.array(id_list)\n",
    "    sentence_list = np.array(sentence_list)\n",
    "    random_index = np.array(range(len(sentence_list)))\n",
    "    random.shuffle(random_index)\n",
    "    id_list = id_list[random_index]\n",
    "    sentence_list = sentence_list[random_index]\n",
    "    \n",
    "    dataframe = pd.DataFrame({'id':id_list,'sentence':sentence_list})\n",
    "    dataframe.to_csv(file_path,index=False,sep='\\t',header=None)\n",
    "    print(len(id_list))\n",
    "    return\n",
    "\n",
    "dev_set_path = 'data/v1/dev_set_v1.txt'\n",
    "train_set_path = 'data/v1/train_set_v1.txt'\n",
    "idx_file_path = 'data/v1/v1_idx.pickle'\n",
    "\n",
    "save_to_file(dev_set_dict, dev_set_path)\n",
    "save_to_file(train_set_dict, train_set_path)\n",
    "\n",
    "\n",
    "# Build IDX and Save\n",
    "idx = {}\n",
    "for i, id in enumerate(train_dict):\n",
    "    idx[id] = i\n",
    "print(len(idx))\n",
    "\n",
    "with open(idx_file_path, 'wb') as handle:\n",
    "    pickle.dump(idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 35437\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"data/test_tweets_unlabeled.txt\"\n",
    "test_output_path = \"data/test_tweets_unlabeled_dataframe.txt\"\n",
    "\n",
    "test_list = []\n",
    "with open(test_file_path, encoding='utf-8') as tsvfile:\n",
    "    reader = tsvfile.readlines()\n",
    "    for i, row in enumerate(reader):\n",
    "        row = row.strip()\n",
    "        row, _ =  pre_process(row, max_length)\n",
    "        test_list.append(row+\"\\n\")\n",
    "with open(test_output_path, 'w') as output_file:\n",
    "    output_file.writelines(test_list)\n",
    "\n",
    "test_list = np.array(test_list)\n",
    "# dataframe = pd.DataFrame({'sentence':test_list})\n",
    "# dataframe.to_csv(test_output_path,index=False,header=None)\n",
    "print(\"Total rows: %d\" % len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0587e884adf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sml/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sml/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sml/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sml/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n"
     ]
    }
   ],
   "source": [
    "test_file = pd.read_csv(test_output_path, header=None)\n",
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
