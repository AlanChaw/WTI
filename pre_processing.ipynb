{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = [\"@handle\", \"RT\", \"http\"]\n",
    "\n",
    "def pre_process(sentence, max_length):\n",
    "    sentence = sentence.split()\n",
    "    target_remove = set()\n",
    "    for token in sentence:\n",
    "        for target in remove_words:\n",
    "            #if (target == \"http\") and (target in token.lower()):\n",
    "            #        target_remove.add(token)\n",
    "            #        break\n",
    "            #if target in token:\n",
    "            #    target_remove.add(token)\n",
    "            #    break\n",
    "            if target in token:\n",
    "                target_remove.add(token)\n",
    "                \n",
    "    for target in target_remove:\n",
    "        sentence.remove(target)\n",
    "    max_length = max(max_length, len(sentence))\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence, max_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Train data to train_dict. \n",
    "train_dict[id] = [[train_instace1], [train_instance2] ....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 328931\n",
      "Total ids: 9296\n",
      "Longest Sentence: 37\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "train_file_path = \"data/train_tweets.txt\"\n",
    "train_dict = collections.defaultdict(list)\n",
    "max_length = 0\n",
    "\n",
    "length_array = []\n",
    "with open(train_file_path, encoding='utf-8') as tsvfile:\n",
    "    #reader = csv.reader((x.replace('\\0', '') for x in tsvfile), delimiter='\\t')\n",
    "    reader = tsvfile.readlines()\n",
    "    for i, row in enumerate(reader):\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        id = int(row[0])\n",
    "        instance, max_length = pre_process(row[1], max_length)\n",
    "        if not instance == \"\":\n",
    "            train_dict[id].append(instance)\n",
    "            length_array.append(len(instance.split()))\n",
    "    print(\"Total rows: %d\" % i)\n",
    "    \n",
    "print(\"Total ids: %d\" % len(train_dict))\n",
    "print(\"Longest Sentence: %d\" % (max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P V U know! Lol! PV 35 G S BO I mean U 32 first time in 21 years keep it up PV! Best BAND IN THE LAND ALL DAY CRAB c / o 98 Stand up!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: [ 8 11  6 ... 12 10 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    327986.000000\n",
       "mean         12.851884\n",
       "std           6.644626\n",
       "min           1.000000\n",
       "25%           8.000000\n",
       "50%          12.000000\n",
       "75%          18.000000\n",
       "max          37.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "length_array = np.array(length_array)\n",
    "sorted_len_array = sorted(length_array)\n",
    "print(\"median: {0}\".format(length_array))\n",
    "s = pd.Series(length_array)\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tweetsPerPersonMap = {}\n",
    "for key in train_dict.keys():\n",
    "    tweetsLen = len(train_dict[key])\n",
    "    if tweetsLen not in tweetsPerPersonMap.keys():\n",
    "        tweetsPerPersonMap[tweetsLen] = [key]\n",
    "    else:\n",
    "        tweetsPerPersonMap[tweetsLen].append(key)\n",
    "\n",
    "sortedTweetsNumList = sorted(tweetsPerPersonMap.keys())\n",
    "sortedTweetsSumList = []\n",
    "userSumList = []\n",
    "for tweetsNum in sortedTweetsNumList:\n",
    "    userNum = len(tweetsPerPersonMap[tweetsNum])\n",
    "    sortedTweetsSumList.append(tweetsNum*userNum)\n",
    "    userSumList.append(userNum)\n",
    "\n",
    "sortedTweetsSumList = np.cumsum(sortedTweetsSumList)\n",
    "userSumList = np.cumsum(userSumList)\n",
    "userSumPropotionList = userSumList/userSumList[-1]\n",
    "sortedTweetsProportionList = sortedTweetsSumList/sortedTweetsSumList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tweets num per person\n",
    "tweets_num_path = 'data/v1_1/tweets_num_per_person.txt'\n",
    "with open(tweets_num_path, 'w') as txtFile:\n",
    "    for index, tweetsNum in enumerate(sortedTweetsNumList):\n",
    "        userList = tweetsPerPersonMap[tweetsNum]\n",
    "        proportion = sortedTweetsProportionList[index]\n",
    "        userProportion = userSumPropotionList[index]\n",
    "        #txtFile.write(\"{0:.2f},{1:.2f},{2},{3}\\n\".format(proportion, userProportion, tweetsNum, userList))\n",
    "        txtFile.write(\"{0:.2f},{1:.2f},{2}\\n\".format(proportion, userProportion, tweetsNum, userList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "# store new dict\n",
    "TWEETS_NUM_THRESHOLD= 55\n",
    "\n",
    "sortedTweetsNumList = np.array(sortedTweetsNumList)\n",
    "newTweetsNumList = sortedTweetsNumList[sortedTweetsNumList>55]\n",
    "\n",
    "print(len(newTweetsNumList))\n",
    "\n",
    "newTrainDict = {}\n",
    "for index, tweetsNum in enumerate(newTweetsNumList):\n",
    "    for user in tweetsPerPersonMap[tweetsNum]:\n",
    "        newTrainDict[user] = train_dict[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132 1132 1132\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "dev_split = 0.1\n",
    "train_split = 1 - dev_split\n",
    "\n",
    "dev_set_dict = {}\n",
    "train_set_dict = {}\n",
    "\n",
    "for id in newTrainDict:\n",
    "    target_list = newTrainDict[id]\n",
    "    length = len(target_list)\n",
    "    random.shuffle(target_list)\n",
    "    split = int(np.ceil(length*dev_split))\n",
    "    dev_set_dict[id] = target_list[:split]\n",
    "    train_set_dict[id] = target_list[split:length]\n",
    "#     print(len(dev_set_dict[id]), len(train_set_dict[id]), length)\n",
    "\n",
    "print(len(dev_set_dict), len(train_set_dict), len(newTrainDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11135\n",
      "95451\n",
      "1132\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_file(target_dict, file_path):\n",
    "    id_list = []\n",
    "    sentence_list = []\n",
    "    for id in target_dict:\n",
    "        for sentence in target_dict[id]:\n",
    "            id_list.append(id)\n",
    "            sentence_list.append(sentence)\n",
    "      \n",
    "    id_list = np.array(id_list)\n",
    "    sentence_list = np.array(sentence_list)\n",
    "    random_index = np.array(range(len(sentence_list)))\n",
    "    random.shuffle(random_index)\n",
    "    id_list = id_list[random_index]\n",
    "    sentence_list = sentence_list[random_index]\n",
    "    \n",
    "    dataframe = pd.DataFrame({'id':id_list,'sentence':sentence_list})\n",
    "    dataframe.to_csv(file_path,index=False,sep='\\t',header=None)\n",
    "    print(len(id_list))\n",
    "    return\n",
    "\n",
    "dev_set_path = 'data/v1_1/dev_set_v1_1_more_than_55.txt'\n",
    "train_set_path = 'data/v1_1/train_set_v1_1_more_than_55.txt'\n",
    "idx_file_path = 'data/v1_1/v1_idx_more_than_55.pickle'\n",
    "\n",
    "save_to_file(dev_set_dict, dev_set_path)\n",
    "save_to_file(train_set_dict, train_set_path)\n",
    "\n",
    "\n",
    "# Build IDX and Save\n",
    "idx = {}\n",
    "for i, id in enumerate(newTrainDict):\n",
    "    idx[id] = i\n",
    "print(len(idx))\n",
    "\n",
    "with open(idx_file_path, 'wb') as handle:\n",
    "    pickle.dump(idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@handle Hahaha!', 'You look like Dave Vanian now? =D', \"Oh we renamed Zero, Seamus O'Malley - he looks like he should have a newsboy cap on & a pint of Guinness by his side...\", 'Dang, I keep missing all of the #bpal contests - I need a pick me up - this week already = fail.', \"I'm hungry! Just sayin'!\", 'My cousin said, \"Sea Mus\" - I know that\\'s not unusual to think that but it made me chuckle...', 'And Elena is still in the hospital too, obviously not as bad off as my dad, but my mom told me she & dad passed each other getting CT scans.', 'Most #atheists know the truth about Mother Teresa anyway...', '& esp. if they live in poverty, fuck the parents & child b/c the child ONLY mattered when it was a mass of cells in a womb. Wow - so smart.', 'Just saw it! Thank you, Tired & not in any mood to \"talk\" - at least I\\'m feeling more human today...*hugs*', 'BTW, J ?s his lil knitted monster, Toby, Sascha will bat it to the floor & J will go get it & put it back up on the clock.', \"I hope Squeegee & kitty (forgot its name!) are getting along! &lt;3 Sascha & mom's cat, Boots, hate each other; Glad we're dwnstrs.\", 'Btw: My brother, a \"proud\" meateater - ate tofurkey and was none the wiser - ha ha ha. I didn\\'t spill the beans \\'cause he\\'d be smug.', \"@handle I don't know because J is the house husband; I don't do a thing really! I'm so awful. I mean I do, but he does more!\", \"That NE album has one of my fave songs of theirs on it and My copy of Xymox's Headclouds was Fux0red. You're the best! <3 <3\", \"Dad update for anyone who cares: He's stable, death certificate was waiting for him in ER when ambulance arrived even though he was alive.\", '@handle Awww! Squeegee does great tricks! =D', \"@handle *SMOOCHES* to my best girls. ? 's you both so. Just saying.\", '- Elena is in the hospital too - 2 floors under my dad. Poor sweetie!', '- Pumpkin beer & my two faves!! Thank you Whole Foods in Chapel Hill!!!!', 'has discovered the wonderful world of #BPAL via Cthulu. Scientist & Athiest + BPAL = LOVE. &lt;3 Someone should send him a bottle!', '@handle J says, \"can I run through the middle of this convo nekkid?????\"', 'Pumpkin Cheesecake for the win!! Yum! =D', 'Seriously people YOUR = ownership, as in YOUR DOG. YOU\\'RE = YOU ARE, as in You\\'re going insane - get it, \"you are going insane.\" Gah!', 'We took a meal to my daddy in the nursing home - and he gobbled (ha ha) it down - even the tofurkey. J made everything & it was delicious!', 'Ok, done for the day with internet. Too tired to deal.', 'To those leaning right, did you ever read anything about Eugenics? How about Eugenics in the USA? It happened.', 'Dad had seizures start-ambulance took him to hospital-situation dire i am afraid.', \"J put up all of my mother's Xmas decorations - it looks like Santa exploded - I'll have to take a pic at some point tonight - no words.\", \"Haha, he'd love that... ;) Glad I don't have one! That keeps me from getting on Skype too - phew!\", 'Also, I has teh gas. I blame the stuffing. Was that TMI? OH WELLS!', 'In somewhat unrelated news, Happy-being-a-daddy-and-much-baby-love to & his wife. #babyfidalgo', 'Because it is! =D', \"Tofurkey - anyone who says it tastes bad apparently can't make a Turkey taste good or anything else for that matter!\", \"*nod* Peter is a mad genious; just sad he lost the patent and now everyone makes 'em; his purses were the best! :(\", \"So they're still not sure what's wrong with Elena - so tonight I'll be visiting both she & my father on diff. floors - I'm not there today.\", 'I will have a brand new baby girl in about 12 hours from now. | Congrats early Ben!!! <3', 'Not sure if he will survive the night...heartbroken. #h1n1', 'Nice surprise! Earlier today I found out Von Erickson is on Etsy! I LOVE my metal coffin purses! Old school goth for the win!', '#STUPAK = #STUPID. Please \"abort\" yourselves, STUPAK supporters. Kthnx. I\\'m tired of your religious stupidity. #atheism #hcr', 'Ni Ni Maya ? you *smooch*', \"It's to my shoulders but it's driving me batty b/c it's just boring!!!!!!!! :/\", '9-9-9: The Number of the Beatles -- | for @handle & @handle - a few days late but still!', 'Mooooooooooooooooooooo.', \"@handle Awwwww! I wrotes in the FaceBook that I was thankfuls for yous ladies on TDay. I am. You're both total tops! ?\", 'W E I R D! but not really. ;)', \"*nod* It's just the damn paypal & ebay fees eat me alive. I miscalculated S&H for one shipment & I had to bite it too...\", 'Amanda Palmer (of The Dresden Dolls) and Nervous Cabaret will play The ArtsCenter on Friday, November 20th', \"Happy Thanksgiving peeps! J is making all the food. Everything smells effing fabulous. I'm still in my PJs.\", 'Ok, I\\'m over the idiotic & stupid. Off to sell my clothing, or try, for funds because $$$ is everyone\\'s real, true \"God\".', \"Apparently dad is going to have an endoscopy - he's still spitting up blood & they're going to give him 2 pints of it today.\", 'Awwww! Hope Lo feels better fast! Candy canes, candy corn, cotton candy, & syrup! ?', \"Dear you totally did not have to do that but it is UBER appreciated - you've no idea. no idea. <3 <3 <3 <3 <3\", 'Father is doing bad. H1N1 is kicking his arse. Age 65 + spinal cord injury = worse problems. Meanwhile mom is bat shit insane.', \"My Democrat friends, please fill me in tomorrow on the goings on b/c I'm on my way to bed; over tonight & people in general Love ya'll tho.\", 'My mother told J he could paint him...HA HA HA - J said to me, we could make him Damien - paint a lil \"666\" in his hair & make his eyes red.', 'Church of Euthanasia could use a few good, cold bodies.', 'Not sure exactly what the test results will show. Not sure if he had a mild heart-attack or not even though they d (cont)', \"Mischka has an irritated rectum. J gave her applesauce mixed with Olive Leaf. Nature's cure we hope.\", 'I did too. Lots of onions, carrots, potatoes, sweet potatoes, fresh cranberry sauce with orange & apple, & tofurkey.', \"I would really love some Starbucks coffee (b/c it's the only decent coffee place in this one horse town) & I has no money = suck.NEED COFFEE\", \"I've photographed a gaggle of clothing & pulled out a metal coffin purse to sell along with a pair of Gripfast 30 eye (modified to 24) boots\", \"Thank you for trying but I am in no mood to talk on the phone - I'm exhausted physically & emotionally.\", 'I second that!', 'The pious princess masturbates & God watches.', \"Virginia & North Carolina's state governments allowed sterilization for ppl that were mainly black & poor & autistic AND White.\", 'I feel ick. I ate too many cookies & drank too much soy nog & rum.', '@handle You should totally video the cats doing tricks - so cute!', 'Well, at least my family has three atheists in it - I can somewhat get through the holidaze w/out drinking too much!', 'They are probably the ones who also consider them goths - I hate them so, hate hate hate.', 'and also win.', 'Dear Scientology loons & goons: Your TV commercials have become more annoying than the Latter Day Saints commercials. Thnx for that! Not.', \"I'm sorry that tweet was just wayyyyyyy too funny!\", 'People are indeed stupid.', 'Pro-Life\\'ers & ppl who call themselves \"Pro-Choice\" but don\\'t \"BELIEVE\" in abortion are so caught up in make-believe ppl. in the sky', \"@handle As long as it's close to feeding time, she does that & is just happy to have her bowl full until she's ready to eat!\", \"@handle Dogs are ridiculous. Sascha just meows at us until one of us gets up...usually J, I'm lazy. =D\", \"@handle That's funny b/c Mischka will sit & stamp her feet& *cry* until she gets food in her bowl, even when she's not hungry\", 'I meant to email you about that earlier! I swear my brain was in academic mode all day today! YES! THANK YOU SO MUCH!', 'About what? I\\'ve lost my temper today about the #abortion debate and health care \"reform\" - I\\'m sick of it all. :/', 'LOL!', \"Dear Rob Pattinson - I admit it, you're one effing hot guy. I watched Twilight again today. *swoon* Just saying!\", 'Gov. Elect Bob McDonnell \"Public option not good for Virginia\" - really?', 'And Paul doesn\\'t have to worry about my hunger b/c The Brides said it best: \"Baby Girls Are Much More Tender\".', \"Ppl. keep calling mother about my father & she puts on this fake cheery act; off phone she's a nervous wreck & i'm waiting for her to break.\", 'Some of my fave concerts from 1999/2000 were Apop concerts! =D And your blog has a great name! Yay for ADD! ;)', 'I keep reading that Eric Cantor is paying a lot of money for a speech coach. Who is that speech coach, Thirstin Howl III?', '- Bad band advertisement but saw it in Danville while in from Chapel Hill.', 'a;lfdkjal;sfjksafljk', 'Yikes! =D We had FIVE freaking dogs here on Turkey Day - gah! It was insanity - PLUS the two cats. Doggie politics...ugh.', \"I paid my bills & now I am teh brokes again. I'll be doing ebay again tomorrow. =D That's my job now apparently.\", 'Is someone walking Rep. Cao home tonight? I hope so. #protectcao #hcr || Had to RT that b4 bed - LOL. <3', \"I need my hair did. S'rsly. I don't know how much longer I can hold out!\", \"The best part? The nativity scene. There's a blonde haired, pale, blue-eyed baby Jesus. It's actually a NICE nativity set, not cheap.\", \"Last year Mary was dropped & broken & I cannot afford to replace her for mom - so one of the Magi is filling in. It's a gay nativity.\", 'I am full of sacrilege today.', \"Eek about the concussion! I'm glad you're ok-I had a friend's dog knock me in the head with his snout once & had that happen!\", 'Dear UNC if you could give me my refund - it would be \"totally awesome\" right now - kthnx.', '- New Message', 'I use logic, analysis, & reason - not emotion & anecdotal evidence. FA!']\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[4032])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dev Train\n",
    "!! There are Some ID have 0 train instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9296 9296 9296\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "dev_split = 0.1\n",
    "train_split = 1 - dev_split\n",
    "\n",
    "dev_set_dict = {}\n",
    "train_set_dict = {}\n",
    "\n",
    "for id in train_dict:\n",
    "    target_list = train_dict[id]\n",
    "    length = len(target_list)\n",
    "    random.shuffle(target_list)\n",
    "    split = int(np.ceil(length*dev_split))\n",
    "    dev_set_dict[id] = target_list[:split]\n",
    "    train_set_dict[id] = target_list[split:length]\n",
    "#     print(len(dev_set_dict[id]), len(train_set_dict[id]), length)\n",
    "\n",
    "print(len(dev_set_dict), len(train_set_dict), len(train_dict))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build ID idx and train/dev set save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36124\n",
      "291862\n",
      "9296\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_file(target_dict, file_path):\n",
    "    id_list = []\n",
    "    sentence_list = []\n",
    "    for id in target_dict:\n",
    "        for sentence in target_dict[id]:\n",
    "            id_list.append(id)\n",
    "            sentence_list.append(sentence)\n",
    "      \n",
    "    id_list = np.array(id_list)\n",
    "    sentence_list = np.array(sentence_list)\n",
    "    random_index = np.array(range(len(sentence_list)))\n",
    "    random.shuffle(random_index)\n",
    "    id_list = id_list[random_index]\n",
    "    sentence_list = sentence_list[random_index]\n",
    "    \n",
    "    dataframe = pd.DataFrame({'id':id_list,'sentence':sentence_list})\n",
    "    dataframe.to_csv(file_path,index=False,sep='\\t',header=None)\n",
    "    print(len(id_list))\n",
    "    return\n",
    "\n",
    "dev_set_path = 'data/v1/dev_set_v1.txt'\n",
    "train_set_path = 'data/v1/train_set_v1.txt'\n",
    "idx_file_path = 'data/v1/v1_idx.pickle'\n",
    "\n",
    "save_to_file(dev_set_dict, dev_set_path)\n",
    "save_to_file(train_set_dict, train_set_path)\n",
    "\n",
    "\n",
    "# Build IDX and Save\n",
    "idx = {}\n",
    "for i, id in enumerate(train_dict):\n",
    "    idx[id] = i\n",
    "print(len(idx))\n",
    "\n",
    "with open(idx_file_path, 'wb') as handle:\n",
    "    pickle.dump(idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = 'data/v1/train_set_v1.txt'\n",
    "file_data = pd.read_csv(temp_file, delimiter=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5592</td>\n",
       "      <td>Do you enjoy the games you are playing in life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8940</td>\n",
       "      <td>Most fun part of owning team. Charging court a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5149</td>\n",
       "      <td>WSJ: Fannie, Freddie Woes Hurt Apartments #rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4562</td>\n",
       "      <td>45% of execs, managers spend 3+ hours a day us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8061</td>\n",
       "      <td>loves audiobooks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6967</td>\n",
       "      <td>A reason to watch Survivor?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8905</td>\n",
       "      <td>Love the new Mastercard commercials with Peyto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6925</td>\n",
       "      <td>Expose to the right - - #photog (via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9431</td>\n",
       "      <td>awake with good strong rich coffee... the temp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5453</td>\n",
       "      <td>Actually saw this last week. Caused some brain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6843</td>\n",
       "      <td>true...except for weed cases.... #unfair and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5268</td>\n",
       "      <td>Well you know...I dig the outdoors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8359</td>\n",
       "      <td>- On my way 2 the European VMAs...1st few feet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1251</td>\n",
       "      <td>I have 100 paperclips just for you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>325</td>\n",
       "      <td>The internet makes rappers talk too much when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6833</td>\n",
       "      <td>Oh I had some last night, i've been recovering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2088</td>\n",
       "      <td>Physical Therapist - Physical Therapy - New Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6028</td>\n",
       "      <td>Cellphone radiation levels vary widely, watchd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4562</td>\n",
       "      <td>Mag story on our recent survey on headcount go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8240</td>\n",
       "      <td>die spatte van het beeldscherm af!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1463</td>\n",
       "      <td>@handle yea I will look at it soon thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4007</td>\n",
       "      <td>New blog post :: Mochi Labels http://bit.ly/2H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2882</td>\n",
       "      <td>Tired of thick wallets that don't hold many ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3679</td>\n",
       "      <td>Thx 4 ur support!!! Will tell everybody tmrw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9148</td>\n",
       "      <td>-- Shouldn't you be asleep by now? GameDay sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6637</td>\n",
       "      <td>If you have a pulse you can make a difference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2236</td>\n",
       "      <td>Dude! I do what does for free. You should see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1581</td>\n",
       "      <td>It's late...uh...early! Had a war tonight with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8466</td>\n",
       "      <td>this is sooooo wrong!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4038</td>\n",
       "      <td>Assuming you're referring to me, and how embar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291832</th>\n",
       "      <td>3290</td>\n",
       "      <td>is booking Europe for a quick turn in Jan/Feb ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291833</th>\n",
       "      <td>7833</td>\n",
       "      <td>Just saw this on Amazon: 'Kindle Wireless Read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291834</th>\n",
       "      <td>3995</td>\n",
       "      <td>Everyone Follow My Homie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291835</th>\n",
       "      <td>8521</td>\n",
       "      <td>Ex-astronaut Lisa Nowak pleads guilty to lesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291836</th>\n",
       "      <td>4897</td>\n",
       "      <td>well they are deep in an emotional scene, so h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291837</th>\n",
       "      <td>6172</td>\n",
       "      <td>Johnson Rose Revitalizes Restaurant Supplies W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291838</th>\n",
       "      <td>9668</td>\n",
       "      <td>Blog - Good Reasons to Get One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291839</th>\n",
       "      <td>8050</td>\n",
       "      <td>LA: Load up at our sample sale featuring on 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291840</th>\n",
       "      <td>9846</td>\n",
       "      <td>Are there any career fields left that *aren't*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291841</th>\n",
       "      <td>2481</td>\n",
       "      <td>Since I'm too impatient to wait til 13,000 fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291842</th>\n",
       "      <td>8271</td>\n",
       "      <td>yeah I was wondering. Thanks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291843</th>\n",
       "      <td>8554</td>\n",
       "      <td>birds the word!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291844</th>\n",
       "      <td>44</td>\n",
       "      <td>2 points from an 80 ahhhh 2 point from getting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291845</th>\n",
       "      <td>5140</td>\n",
       "      <td>I know.he took me there the 1st time I went up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291846</th>\n",
       "      <td>1849</td>\n",
       "      <td>For which venue and for which night please?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291847</th>\n",
       "      <td>5521</td>\n",
       "      <td>sharing my latest post: Pay Attention Namasté!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291848</th>\n",
       "      <td>4612</td>\n",
       "      <td>Lady Gaga Premieres “Bad Romance,” Her Crazies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291849</th>\n",
       "      <td>1876</td>\n",
       "      <td>Really dig 'this masquerafe' &gt;&gt; Demos from Mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291850</th>\n",
       "      <td>2663</td>\n",
       "      <td>Suit yourself, kinda weird to do that over one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291851</th>\n",
       "      <td>8248</td>\n",
       "      <td>Shout out 2 all the men taking care of kids th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291852</th>\n",
       "      <td>6857</td>\n",
       "      <td>is reminding you to bring out your own disks, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291853</th>\n",
       "      <td>503</td>\n",
       "      <td>Finally made it to texas!!! Had some dinner an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291854</th>\n",
       "      <td>3764</td>\n",
       "      <td>Irony loves company: The RNC's own insurance p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291855</th>\n",
       "      <td>7720</td>\n",
       "      <td>#MM Rush - Xanadu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291856</th>\n",
       "      <td>4362</td>\n",
       "      <td>Has a niece! Mallory Grace Lasseigne! 9 lbs 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291857</th>\n",
       "      <td>6977</td>\n",
       "      <td>Had so much fun last nite.. Wes &amp; Jess crack m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291858</th>\n",
       "      <td>2416</td>\n",
       "      <td>3.7 earthquake rattles Big Bear Lake area Sund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291859</th>\n",
       "      <td>4069</td>\n",
       "      <td>GM: All Out of Scapegoats - Now What?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291860</th>\n",
       "      <td>6084</td>\n",
       "      <td>Physician - Cardiac Surgery First Assist-Physi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291861</th>\n",
       "      <td>9909</td>\n",
       "      <td>Just entered a beat i hope i win.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291862 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                                                  1\n",
       "0       5592  Do you enjoy the games you are playing in life...\n",
       "1       8940  Most fun part of owning team. Charging court a...\n",
       "2       5149  WSJ: Fannie, Freddie Woes Hurt Apartments #rea...\n",
       "3       4562  45% of execs, managers spend 3+ hours a day us...\n",
       "4       8061                                   loves audiobooks\n",
       "...      ...                                                ...\n",
       "291857  6977  Had so much fun last nite.. Wes & Jess crack m...\n",
       "291858  2416  3.7 earthquake rattles Big Bear Lake area Sund...\n",
       "291859  4069              GM: All Out of Scapegoats - Now What?\n",
       "291860  6084  Physician - Cardiac Surgery First Assist-Physi...\n",
       "291861  9909                  Just entered a beat i hope i win.\n",
       "\n",
       "[291862 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
